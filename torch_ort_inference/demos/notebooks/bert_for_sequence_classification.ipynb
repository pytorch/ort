{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QafPjQEIua55"
      },
      "source": [
        "Copyright (C) 2022, Intel Corporation\n",
        "\n",
        "SPDX-License-Identifier: MIT License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWjYJLLzGbpp"
      },
      "source": [
        "# Sequence Classification with BERT in Python using torch-ort inference module:\n",
        "\n",
        "1. This demo shows how to use Intel® OpenVINO™ integration with Torch-ORT to check grammar in text with ONNX Runtime OpenVINO Execution Provider.\n",
        "\n",
        "2. We use a sequence classification model textattack/bert-base-uncased-CoLA from HuggingFace models. This model is trained on the BERT architecture to check grammar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMnOpUDbu1Rn"
      },
      "source": [
        "### Import Necessary Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf9XWpsSaiof"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "import torch\n",
        "from torch_ort import ORTInferenceModule, OpenVINOProviderOptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpRiieBmuv6k"
      },
      "source": [
        "### Preprocess Function\n",
        "* Use `encode` to:\n",
        "  * Tokenize the sentence\n",
        "  * Map tokens to their IDs.\n",
        "* Pad our input tokens with value 0\n",
        "* Truncate inuput to MAX_LEN\n",
        "* Create the attention mask.\n",
        "* Store the input ids and attention masks for the sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHW8XFJ4k7ys"
      },
      "outputs": [],
      "source": [
        "def preprocess_input(tokenizer, sentences):\n",
        "    # Tokenization & Input Formatting\n",
        "    # Config: \"do_lower_case\": true, \"model_max_length\": 512\n",
        "    inputs = []\n",
        "\n",
        "    MAX_LEN = 64\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # `encode` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        encoded_sent = tokenizer.encode(\n",
        "                            sentence,                      # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                    )\n",
        "\n",
        "        # Pad our input tokens with value 0.\n",
        "        if len(encoded_sent) < MAX_LEN:\n",
        "            encoded_sent.extend([0]*(MAX_LEN-len(encoded_sent)))\n",
        "\n",
        "        # Truncate to MAX_LEN\n",
        "        if len(encoded_sent) > MAX_LEN:\n",
        "            print(\"WARNING: During preprocessing, number of tokens for the sentence {}\"\\\n",
        "                \"exceedeed MAX LENGTH {}. This might impact accuracy of the results\".format(\n",
        "                sentence,\n",
        "                MAX_LEN\n",
        "            ))\n",
        "            encoded_sent = encoded_sent[:MAX_LEN]\n",
        "\n",
        "        # Create the attention mask.\n",
        "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "        att_mask = [int(token_id > 0) for token_id in encoded_sent]\n",
        "\n",
        "        # Store the input ids and attention masks for the sentence.\n",
        "        inputs.append({'input_ids': torch.unsqueeze(torch.tensor(encoded_sent),0),\n",
        "                'attention_mask': torch.unsqueeze(torch.tensor(att_mask),0)})\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVbaBmJD0BfE"
      },
      "source": [
        "### Inference Function\n",
        "* Runs the inference on the input sentences\n",
        "* Prints the inference results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hyq5KoPzk8dU"
      },
      "outputs": [],
      "source": [
        "def infer(model, sentences, inputs):\n",
        "    num_sentences = len(sentences)\n",
        "    total_infer_time = 0\n",
        "    results = {}\n",
        "\n",
        "    # Run inference\n",
        "    for i in range(num_sentences):\n",
        "        input_ids = (inputs[i])['input_ids']\n",
        "        attention_masks = (inputs[i])['attention_mask']\n",
        "        with torch.no_grad():\n",
        "            # warm-up\n",
        "            if i == 0:\n",
        "               t0 = time.time()\n",
        "               model(input_ids, attention_masks)\n",
        "            # infer\n",
        "            t0 = time.time()\n",
        "            outputs = model(input_ids, attention_masks)\n",
        "            t1 = time.time() - t0\n",
        "            total_infer_time += t1\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Move logits\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "\n",
        "        # predictions\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "        orig_sent = sentences[i]\n",
        "        results[orig_sent] = pred_flat[0]\n",
        "\n",
        "    print(\"\\n Number of sentences: {}\".format(num_sentences))\n",
        "    if num_sentences > 20:\n",
        "        print(\" First 20 results:\")\n",
        "    print(\"\\t Grammar correctness label (0=unacceptable, 1=acceptable)\\n\")\n",
        "    count = 0\n",
        "    for k, v in results.items():\n",
        "        print(\"\\t{!r} : {!r}\".format(k, v))\n",
        "        if count == 20:\n",
        "            break\n",
        "        count = count + 1\n",
        "    print(\"\\n Average inference time: {:.4f}ms\".format((total_infer_time/num_sentences)*1000))\n",
        "    print(\" Total Inference time: {:.4f}ms\".format(total_infer_time * 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H61DUd1BvIgb"
      },
      "source": [
        "### Select the inputs\n",
        "\n",
        "* Use `sentences` to select your inputcsentences to be inferred\n",
        "\n",
        "* Available backend precisions\n",
        "  * CPU: FP32\n",
        "  * GPU(Does not work in collab): FP32, FP16\n",
        "\n",
        "* Available inference execution providers \n",
        "  * OpenVINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0fY7Pokk8o9"
      },
      "outputs": [],
      "source": [
        "sentences = [\"This is a BERT sample.\",\"User input is valid not.\"]\n",
        "\n",
        "backend =\"CPU\"\n",
        "precision = \"FP32\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouvQOafYu9AP"
      },
      "source": [
        "### Get and load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY7qLpvoqtcf"
      },
      "outputs": [],
      "source": [
        "# Load Model\n",
        "# Pretrained model fine-tuned on CoLA dataset from huggingface model hub to predict grammar correctness\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"textattack/bert-base-uncased-CoLA\"\n",
        ")\n",
        "\n",
        "# Load Tokenizer & Preprocess input sentences\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-CoLA\")\n",
        "inputs = preprocess_input(tokenizer, sentences)\n",
        "\n",
        "\n",
        "# Convert model for evaluation\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0RQWT2gwM1v"
      },
      "source": [
        "### Run the inference with native PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpQZQ5RKq2N4",
        "outputId": "25b82f86-5e50-4385-a8a3-b75f11518b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Number of sentences: 2\n",
            "\t Grammar correctness label (0=unacceptable, 1=acceptable)\n",
            "\n",
            "\t'This is a BERT sample.' : 1\n",
            "\t'User input is valid not.' : 0\n",
            "\n",
            " Average inference time: 280.6635ms\n",
            " Total Inference time: 561.3270ms\n"
          ]
        }
      ],
      "source": [
        "# Infer\n",
        "infer(model, sentences, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UfBmOlVwmPZ"
      },
      "source": [
        "### Run the inference with Torch-ORT inference module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRm9_BYrk8uW",
        "outputId": "58e0d8e6-f034-4b24-fb6d-655d81f51f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Number of sentences: 2\n",
            "\t Grammar correctness label (0=unacceptable, 1=acceptable)\n",
            "\n",
            "\t'This is a BERT sample.' : 1\n",
            "\t'User input is valid not.' : 0\n",
            "\n",
            " Average inference time: 236.1443ms\n",
            " Total Inference time: 472.2886ms\n"
          ]
        }
      ],
      "source": [
        "# Select OpenVINO as inference execution provider\n",
        "if backend and precision:\n",
        "    provider_options = OpenVINOProviderOptions(backend, precision)\n",
        "    model = ORTInferenceModule(model, provider_options=provider_options)\n",
        "else:\n",
        "    model = ORTInferenceModule(model)\n",
        "\n",
        "# Infer\n",
        "infer(model, sentences, inputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bert_for_sequence_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
